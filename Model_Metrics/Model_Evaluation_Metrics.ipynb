{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    correct_counter = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == yp:\n",
    "            correct_counter += 1\n",
    "    return correct_counter / len(y_true)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "metrics.accuracy_score(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive : Model predicts the image belongs to a class and the image does belongs to the class.\n",
    "\n",
    "True Negative: Model predicts the image does not belongs to the class and the image actually does not belong to the class.\n",
    "\n",
    "###### Basically the model correctly predicts positive class, it is true positive,and if the mode accurately predicts negative class, it is a true negative.\n",
    "\n",
    "False positve: Model predicts the image belongs to a class but the image does not belong to the class.\n",
    "\n",
    "False Negative: Model predicts the image belongs does not to a class but the image does belong to the class.\n",
    "\n",
    "###### Basically, if the model incorrectly predicts positive class, it is a false positive. If the model incorrectly predicts negative class, it is a false negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt,yp in zip(y_true,y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 2\n",
      "True Negative: 3\n",
      "False Positive: 1\n",
      "False Negative: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"True Positive:\",true_positive(l1,l2))\n",
    "print(\"True Negative:\",true_negative(l1,l2))\n",
    "print(\"False Positive:\",false_positive(l1,l2))\n",
    "print(\"False Negative:\",false_negative(l1,l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy Score: ( TP / TN ) / ( TP + TN + FP + FN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_2(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "0.625\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(l1,l2))\n",
    "print(accuracy_2(l1,l2))\n",
    "print(metrics.accuracy_score(l1,l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision = TP / ( TP + FP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true,y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true,y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall =  TP / ( TP + FN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true,y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    recall = tp / ( tp + fn )\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score is a metric that combines both precision and recall. It is defined harmonic mean ( weighted average ) of precision and recall. \n",
    "\n",
    "#### 1. F1 = 2PR / ( P +R )  \n",
    "#### 2. F1 = 2TP / ( 2TP + FP + FN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true,y_pred):\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    \n",
    "    score = 2 * p * r / (p + r)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Positive Rate (TPR)\n",
    "#### TPR  = TP / ( TP + FN )\n",
    "###### TPR or recall is also known as sensitivity\n",
    "#### ======================================\n",
    "#### False Positive Rate\n",
    "#### FPR = FP / (TN + FP)\n",
    "##### FPR is known as specificity or True Negative Rate or TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr(y_true,y_pred):\n",
    "    return recall(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under ROC Curve or Area under Curve ( AUC )\n",
    "###### AUC = 1: implies you have a perfect model.\n",
    "###### AUC =0: implies you have a very bad model.\n",
    "###### AUC =0.5: implies that your model predictions are random.\n",
    "\n",
    "##### AUC values between 0 and 0.5 implies that the model is worse than random. AUC values closer to 1 are considered good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300000000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 0, 1, 0, 1,\n",
    "          0, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n",
    "          0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n",
    "          0.85, 0.15, 0.99]\n",
    "\n",
    "metrics.roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC is log loss.\n",
    "###### Log loss for binary classification is\n",
    "log_loss = -1.0 *( target * log(prediction) + (1 - target) * log( 1 - prediction ))\n",
    "#### Log loss penalizes more than other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def log_loss(y_true, y_prob):\n",
    "    epsilon = 1e-15\n",
    "    loss = []\n",
    "    for yt, yp in zip(y_true, y_proba):\n",
    "        yp = np.clip(yp, epsilon, 1 - epsilon)\n",
    "        temp_loss = - 1.0 * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n",
    "        loss.append(temp_loss)\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss Defined:  0.49882711861432294\n",
      "Comparing with sckit-learn\n",
      "Scikit-Learn:  0.49882711861432294\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 0, 1, 0, 1,0, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,0.9, \n",
    "           0.5, 0.3, 0.66, 0.3, 0.2,\n",
    "            0.85, 0.15, 0.99]\n",
    "print(\"Log Loss Defined: \",log_loss(y_true, y_proba))\n",
    "\n",
    "print(\"Comparing with sckit-learn\")\n",
    "\n",
    "print(\"Scikit-Learn: \",metrics.log_loss(y_true, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision for multi-class classification\n",
    "1.<b>Macro averaged precision</b>:calculate precision for all classes individually and then average them.<br>\n",
    "2.<b>Micro averaged precision</b>: calculate class wise true positive and false positive and then use that to calculate overall precision.<br>\n",
    "3.<b>Weighted precision</b>: same as macro but in this case it is weighted average depending on the number of items in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_precision(y_true,y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    precision = 0\n",
    "    for class_ in range(num_classes):\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        tp = true_positive(temp_true,temp_pred)\n",
    "        fp = false_positive(temp_true,temp_pred)\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        precision += temp_precision\n",
    "    precision /= num_classes\n",
    "    return precision\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precision(y_true,y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for class_ in range(num_classes):\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        tp += true_positive(temp_true,temp_pred)\n",
    "        fp += false_positive(temp_true,temp_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_precision(y_true,y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    class_counts = Counter(y_true)\n",
    "    precision = 0\n",
    "    for class_ in range(num_classes):\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        tp = true_positive(temp_true,temp_pred)\n",
    "        fp = false_positive(temp_true,temp_pred)\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        weighted_precision = class_counts[class_] * temp_precision\n",
    "        precision += weighted_precision\n",
    "    overall_precision = precision / len(y_true)\n",
    "    return overall_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using defined fucntion:\n",
      "0.3611111111111111\n",
      "Sklearn metric:\n",
      "0.3611111111111111\n",
      "\n",
      "Using defined fucntion:\n",
      "0.4444444444444444\n",
      "Sklearn metric:\n",
      "0.4444444444444444\n",
      "\n",
      "Using defined fucntion:\n",
      "0.39814814814814814\n",
      "Sklearn metric:\n",
      "0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,2,0,1,2,0,2,2]\n",
    "y_pred = [0,2,1,0,2,1,0,0,2]\n",
    "print(\"Using defined fucntion:\")\n",
    "print(macro_precision(y_true,y_pred))\n",
    "print(\"Sklearn metric:\")\n",
    "print(metrics.precision_score(y_true,y_pred,average=\"macro\"))\n",
    "print()\n",
    "print(\"Using defined fucntion:\")\n",
    "print(micro_precision(y_true,y_pred))\n",
    "print(\"Sklearn metric:\")\n",
    "print(metrics.precision_score(y_true,y_pred,average=\"micro\"))\n",
    "print()\n",
    "print(\"Using defined fucntion:\")\n",
    "print(weighted_precision(y_true,y_pred))\n",
    "print(\"Sklearn metric:\")\n",
    "print(metrics.precision_score(y_true,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1(y_true,y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    class_counts = Counter(y_true)\n",
    "    f1 = 0 \n",
    "    for class_ in range(num_classes):\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        p = precision(temp_true,temp_pred)\n",
    "        r = recall(temp_true,temp_pred)\n",
    "        if p + r != 0:\n",
    "            temp_f1 = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            temp_f1 = 0\n",
    "        weighted_f1 = class_counts[class_] * temp_f1\n",
    "        f1 += weighted_f1\n",
    "    overall_f1 = f1 / len(y_true)\n",
    "    return overall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using defined fucntion:\n",
      "0.41269841269841273\n",
      "Sklearn metric:\n",
      "0.41269841269841273\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,2,0,1,2,0,2,2]\n",
    "y_pred = [0,2,1,0,2,1,0,0,2]\n",
    "print(\"Using defined fucntion:\")\n",
    "print(weighted_f1(y_true,y_pred))\n",
    "print(\"Sklearn metric:\")\n",
    "print(metrics.f1_score(y_true,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Now we have precision, recall, and F1 implemented for multi-class.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
